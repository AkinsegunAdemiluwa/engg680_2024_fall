{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sU4f6NwZ0JnJ","executionInfo":{"status":"ok","timestamp":1733807216388,"user_tz":420,"elapsed":970,"user":{"displayName":"Akolawole Bode-Fakunle","userId":"15138798629468836922"}},"outputId":"b44a9fbb-d535-453f-ef7f-9284d04b4198"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mm4UZpmF0TTr","executionInfo":{"status":"ok","timestamp":1733807218017,"user_tz":420,"elapsed":145,"user":{"displayName":"Akolawole Bode-Fakunle","userId":"15138798629468836922"}},"outputId":"d0e2f044-5ed3-47c7-ee38-f3b325272f59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["%cd engg680_2024_fall"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeBkKXLx0YGs","executionInfo":{"status":"ok","timestamp":1733807219702,"user_tz":420,"elapsed":136,"user":{"displayName":"Akolawole Bode-Fakunle","userId":"15138798629468836922"}},"outputId":"4a2b1c4f-62b1-4691-8474-0b4e6471bc1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/engg680_2024_fall\n"]}]},{"cell_type":"code","source":["%cd Project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhvRmV1N0gBZ","executionInfo":{"status":"ok","timestamp":1733807221198,"user_tz":420,"elapsed":134,"user":{"displayName":"Akolawole Bode-Fakunle","userId":"15138798629468836922"}},"outputId":"6264e79e-0c11-450a-ba93-38a45ddecf0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/engg680_2024_fall/Project\n"]}]},{"cell_type":"markdown","metadata":{"id":"S08tJXcT4NXQ"},"source":["# **ENGG680 - Introduction to Digital Engineering**\n","## ***PROJECT: Automated Detection of Roads and Pathways in Aerial Imagery Using Deep Learning (U-Net Model)***\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d7I0ssSD4NXT"},"source":["## Preliminary: Certificate of Work\n"]},{"cell_type":"markdown","metadata":{"id":"hcfHtIeQ4NXU"},"source":["*We, the undersigned, certify that this is our own work, which has been done expressly for this course, either without the assistance of any other party or where appropriate we have acknowledged the work of others. Further, we have read and understood the section in the university calendar on plagiarism/cheating/other academic misconduct and we are aware of the implications thereof. We request that the total mark for this assignment be distributed as follows among group members:*"]},{"cell_type":"markdown","metadata":{"id":"7EJY9e844NXW"},"source":["|          | First Name | Last Name | Signature (Full Name, Date) | Hours | Contribution % |\n","|----------|------------|-----------|-----------------------------|-------|----------------|\n","| Member 1: | Akinsegun | Ademiluwa | Akinsegun Ademiluwa, 2024-12-09 | 72 Hours | 12.5 |\n","| Member 2: | Ridwan | Sharafadeen | Ridwan Sharafadeen, 2024-12-09 | 72 Hours | 12.5 |\n","| Member 3: | Cyprain | Okafor | Cyprain Okafor, 2024-12-09 |  72 Hours | 12.5 |\n","| Member 4: | Onyeka | Emecheta | Onyeka Emecheta, 2024-12-09 | 72 Hours | 12.5 |\n","| Member 5: | Akolawole | Bode-Fakunle | Akolawole Bode-Fakunle, 2024-12-09 | 72 Hours | 12.5 |\n","| Member 6: | Victor | Fajonyomi | Victor Fajonyomi, 2024-12-09 | 72 Hours | 12.5 |\n","| Member 7: | Genevieve | Aluziwe | Genevieve Aluziwe, 2024-12-09 |  72 Hours | 12.5 |\n","| Member 8: | David | Olubiyi | David Olubiyi, 2024-12-09 | 72 Hours | 12.5 |"]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n","from tensorflow.keras.models import Model\n","from albumentations import HorizontalFlip, VerticalFlip, RandomRotate90, Compose"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BNq0W_tuG92","executionInfo":{"status":"ok","timestamp":1733803355727,"user_tz":420,"elapsed":14113,"user":{"displayName":"Olubiyi Toluwalase","userId":"10531959455756617866"}},"outputId":"e82bda48-c7aa-4815-9902-e071996c8391"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}]},{"cell_type":"code","source":["# Define paths based on your project structure:\n","aerial_folder = \"C:/Users/AL-WASI/Desktop/ML/FUOtuoke/University of Calgary/Aerial\"\n","image_folder = os.path.join(aerial_folder, \"road_path\")\n","img_path = os.path.join(image_folder, \"img.png\")\n","mask_path = os.path.join(image_folder, \"label.png\")\n"],"metadata":{"id":"2-0rEZYAu2w7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load mask function with binary normalization:\n","def load_mask(mask_path):\n","    if not os.path.exists(mask_path):\n","        print(f\"Error: The file at {mask_path} does not exist.\")\n","        return None\n","    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","    if mask is None:\n","        print(\"Error: Failed to load the mask. Check the file path or file integrity.\")\n","        return None\n","    mask = cv2.resize(mask, (256, 256))\n"],"metadata":{"id":"m4vAyRz5whXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    # Binarize mask: set all values greater than 0 to 1 (assuming binary segmentation):\n","    mask[mask > 0] = 1"],"metadata":{"id":"MOM3eIv6wl3x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    # Convert mask to one-hot encoded format\n","    mask_one_hot = to_categorical(mask, num_classes=2)\n","    print(\"Mask one-hot shape:\", mask_one_hot.shape)  # Should be (256, 256, 2)\n","    return mask_one_hot"],"metadata":{"id":"BofJMm2Lwp26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set paths:\n","aerial_folder = \"C:/Users/AL-WASI/Desktop/ML/FUOtuoke/University of Calgary/Aerial\"\n","image_folder = os.path.join(aerial_folder, \"road_path\")\n","img_path = os.path.join(image_folder, \"img.png\")\n","mask_path = os.path.join(image_folder, \"label.png\")"],"metadata":{"id":"eIv9JS6PwwMe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load images and masks:\n","original_image = load_image(img_path)\n","original_mask = load_mask(mask_path)\n","\n","if original_image is not None and original_mask is not None:\n","    print(\"Image and mask loaded and processed successfully.\")\n","else:\n","    print(\"Failed to load image or mask.\")"],"metadata":{"id":"WayumyyDwxVj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Augmentation:\n","def augment_data(image, mask, num_augmented=10):\n","    aug = Compose([\n","        HorizontalFlip(p=0.5),\n","        VerticalFlip(p=0.5),\n","        RandomRotate90(p=0.5)\n","    ])\n","    images, masks = [image], [mask]\n","    for _ in range(num_augmented):\n","        augmented = aug(image=image, mask=mask)\n","        images.append(augmented['image'])\n","        masks.append(augmented['mask'])\n","    return np.array(images), np.array(masks)\n","\n","augmented_images, augmented_masks = augment_data(original_image, original_mask)"],"metadata":{"id":"geqNdcSIz-WN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a subset of augmented images and masks for testing\n","# Assuming you have enough samples, you can set aside some for testing:\n","split_index = int(0.8 * len(augmented_images))  # 80% for training, 20% for testing\n","train_images, test_images = augmented_images[:split_index], augmented_images[split_index:]\n","train_masks, test_masks = augmented_masks[:split_index], augmented_masks[split_index:]"],"metadata":{"id":"dqccYTiZ0IDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Define UNet Model\n","def unet_model(input_size=(256, 256, 3)):\n","    inputs = Input(input_size)\n","\n","    # Down-sampling layers\n","    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n","    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","\n","    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n","    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n","    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","\n","    # Bottleneck\n","    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n","    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n","\n","    # Up-sampling layers\n","    up7 = concatenate([UpSampling2D(size=(2, 2))(conv4), conv3], axis=-1)\n","    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(up7)\n","    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)\n","\n","    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n","    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(up8)\n","    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n","\n","    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n","    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(up9)\n","    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)\n","\n","    outputs = Conv2D(2, (1, 1), activation='softmax')(conv9)\n","\n","    model = Model(inputs=[inputs], outputs=[outputs])\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model"],"metadata":{"id":"5_0zUAOo4DQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize and train the model\n","model = unet_model()\n","\n","# Training the model using the augmented data\n","#history = model.fit(augmented_images, augmented_masks, batch_size=4, epochs=10, validation_split=0.2)\n","# Re-train model with training split (optional if already trained)\n","history = model.fit(train_images, train_masks, batch_size=4, epochs=10, validation_split=0.2)\n"],"metadata":{"id":"qqoYt9oAE7bP"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"PROJ682_ENV","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}